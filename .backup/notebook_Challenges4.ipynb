{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "rag_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is Aaronson guilty?\n",
      "Answer: 예, Aaronson은 그가 기소된 범죄로 유죄입니다.\n",
      "\n",
      "Question: What message did he write in the table?\n",
      "Answer: 그는 다음과 같은 메시지를 테이블에 썼습니다: FREEDOM IS SLAVERY, TWO AND TWO MAKE FIVE, GOD IS POWER.\n",
      "\n",
      "Question: Who is Julia?\n",
      "Answer: Julia는 이 문서에서 언급된 캐릭터 중 하나로, 주인공인 Winston Smith와 사랑을 나누는 여성입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Stuff Documents 체인을 사용하여 완전한 RAG 파이프라인을 구현하세요.\n",
    "체인을 수동으로 구현해야 합니다.\n",
    "체인에 ConversationBufferMemory를 부여합니다.\n",
    "이 문서를 사용하여 RAG를 수행하세요: https://gist.github.com/serranoarevalo/5acf755c2b8d83f1707ef266b82ea223\n",
    "체인에 다음 질문을 합니다:\n",
    "Aaronson 은 유죄인가요?\n",
    "그가 테이블에 어떤 메시지를 썼나요?\n",
    "Julia 는 누구인가요?\n",
    "\"\"\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# 1. 문서 로드 (Document Loading)\n",
    "loader = TextLoader(\"./files/1984_gist.txt\", encoding='utf-8')\n",
    "\n",
    "\n",
    "# 2. 문서 쪼개기 (Document Splitting, CharacterTextSplitter)\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "# 3. 임베딩 생성 및 캐시 (OpenAIEmbeddings, CacheBackedEmbeddings)\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")\n",
    "\n",
    "# 4. 벡터 스토어 생성 (FAISS)\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 5. 대화 메모리와 질문 처리 (ConversationBufferMemory)\n",
    "memory = ConversationBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(document.page_content for document in docs)\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "# 6. 체인 연결 \n",
    "map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim. If there is no relevant text, return : ''\n",
    "            -------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "    return \"\\n\\n\".join(\n",
    "        map_doc_chain.invoke(\n",
    "            {\"context\": doc.page_content, \"question\": question}\n",
    "        ).content\n",
    "        for doc in documents\n",
    "    )\n",
    "\n",
    "map_chain = {\n",
    "    \"documents\": retriever,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnableLambda(map_docs)\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "            please answer in Korean.\n",
    "            ------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": map_chain,  # map_chain이 문맥 추출을 담당\n",
    "        \"question\": RunnablePassthrough(),  # 질문 그대로 전달\n",
    "        \"chat_history\": RunnableLambda(load_memory),  # 대화 메모리 로드\n",
    "    }\n",
    "    | final_prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "def invoke_chain(question):\n",
    "    print(f\"Question: {question}\")\n",
    "    result = chain.invoke(question)\n",
    "    memory.save_context({\"input\": question}, {\"answer\": result.content})\n",
    "    print(f\"Answer: {result.content}\\n\")\n",
    "\n",
    "\n",
    "invoke_chain(\"Is Aaronson guilty?\")\n",
    "invoke_chain(\"What message did he write in the table?\")\n",
    "invoke_chain(\"Who is Julia?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
